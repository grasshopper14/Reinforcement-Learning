{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning - Disrete and Continuous maps with Q-learning\n",
    "\n",
    "In the previous blog, we got introduced to the concept of reinforcement learning through an example (MountainCar-v0) of OpenAI's gym. The problem was to build a model that will decide the action taken by a Mountain car such that the car reaches its target -  a flag. Each state of the car has two variables representing instantaneous distance and velocity of the car. The target was located at a distance $s=0.5$.\n",
    "\n",
    "We built a model by running $10^4$ sessions. The actions taken at various states were random. And we selected the top $1\\%$ of the most rewarding sessions that had maximum $s$ close to $0.5$, as the training set. The model was built using neural network with two hidden layers. The actions determined by the model were evaluated to be successful.\n",
    "\n",
    "From this exercise, the questions that can bother us are: \n",
    "- If we are to run so many number of sessions to build a model, we have to store the states and outcomes. What if this leads to problems related to memory?\n",
    "- What if running each session is expensive in an application such as medical diagnosis or a space rover that will have to learn its best actions on the run? \n",
    "\n",
    "### Alternative approach\n",
    "\n",
    "To address the above questions let us re-examine our approach. We know that the rewarding action is something that can move the car closer to the target $s=0.5$. The best action in a particular state results in maximum displacement of the car from its current state. The measure of reward $r$ is therefore set as $|s_{old}-s_{new}|$. \n",
    "\n",
    "Let us define a quantity, $Q$ that represents expected future rewards from a session. This expected future rewards $Q$ has to be maximized.\n",
    "\n",
    "We will require some hyperparameters for the following reasons:\n",
    "\n",
    "- If the car keeps oscillating about a point, the expected future rewards will keep increasing. This can lead to infinite sum and $Q\\rightarrow \\infty$. To avoid this we need to include something that can make infinite sums finite. A geometric parameter $\\gamma(<1)$ will be of help in this situation.\n",
    "\n",
    "- Since the model keeps learning it is wise to give more value to recent steps when compared to past computation. Therefore, a learning step parameter $\\alpha$ is introduced.\n",
    "\n",
    "With these hyperparamters, the desired equation is:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big[r_t + \\gamma \\max\\limits_{a} Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\\big]$$\n",
    "\n",
    "The required condition is $Q\\rightarrow r+\\gamma \\max Q,$ which is the sum of current reward and the maximum of expected discounted future rewards. This condition can be met iteratively. $Q$ is introduced formally through Bellman optimality equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways of applying Q-learning\n",
    "\n",
    "Let us apply the above approach in two ways to make the car reach the flag.\n",
    "\n",
    "- We can discretize the state and build a q-table. In this problem, states with negative and positive velocities are categorized as two separate states. We already know that there are three actions in this Mountain car example. The car may move left, right or it may stop. Three actions and two categories of state will result in a $Q$-table of dimension 2x3.\n",
    "\n",
    "- We can build a neural network as a regression model that can accomodate infinitely many states. This will take a continous set of floating point values and provide $Q$, which will be maximum for the appropriate action. Therefore, the index of the maximum will provide us the desired action.\n",
    "\n",
    "One more thing we should keep in mind. In this reinforcement learning approach we are trying to learn as we generate data. The model has to make random choices before learning to make right decisions. This can be achieved by introducing an exploration-exploitation constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a Q-table\n",
    "\n",
    "The states are discretized into two parts. One with a positive velocity and the other with a negative velocity. A Q-table that provides values at two states with rewards from three actions (left, stop and right). We initialize the table fo dimension 2x3 with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Reached the flag in 10/10 games\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "def discretize(s):\n",
    "    if s[1]<0:\n",
    "        return 0\n",
    "    return 1\n",
    "def q_learning_table(env, num_episodes=100):\n",
    "    q_table = np.zeros((2,3))\n",
    "    gamma = 0.95 # discount rate\n",
    "    lr = 0.1 # learning rate\n",
    "    eps = 0.01 #exploration-exploitation constant\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[discretize(s),:]) == 0 or np.random.random()<eps:\n",
    "                a = np.random.randint(0, 3)\n",
    "            else:\n",
    "                a = np.argmax(q_table[discretize(s), :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            def_reward = abs(s[0]-new_s[0]) # define a new reward\n",
    "            q_table[discretize(s), a] += def_reward + lr*(gamma*np.max(q_table[discretize(new_s), :]) - q_table[discretize(s), a])\n",
    "            s = new_s\n",
    "    return q_table\n",
    "def eval_model(q_table, numevals=10):\n",
    "    c = 0\n",
    "    for _ in range(numevals):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = np.argmax(q_table[discretize(s)])\n",
    "            s,r,done,_ = env.step(a)\n",
    "            if s[0]>=0.5:            \n",
    "                c+=1\n",
    "                break\n",
    "    return c\n",
    "numevals = 10\n",
    "num_episodes = 1000\n",
    "q_table = q_learning_table(env, num_episodes)\n",
    "print('Reached the flag in {}/{} games'.format(eval_model(q_table),numevals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a Q-model\n",
    "\n",
    "In case we face a situation where we are unable to discretize the states, then we can build a regression model instead of a table. Regression models are continuous maps of the input. It may take more number of episodes to optimize the model and we will use the same two leayer neural network as in the previous blog.\n",
    "\n",
    "As the model optimizes (or learns) the amount of exploration can be reduced. This is done by introducing a decay factor in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "class dqnn(gym.Wrapper):\n",
    "  def __init__(self, env, states = 2, actions = 3, gamma = 0.99, eps = 0.5, \n",
    "               lr = 0.001, episodes = 3000, decay = 0.999):\n",
    "    gym.Wrapper.__init__(self, env)\n",
    "    self.states = states\n",
    "    self.actions = actions\n",
    "    self.gamma = gamma\n",
    "    self.eps = eps\n",
    "    self.learning_rate = lr\n",
    "    self.num_episodes = episodes\n",
    "    self.decay_rate = decay\n",
    "    self.model = self.nn_model()\n",
    "  def nn_model(self):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=self.states, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(self.actions, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr= self.learning_rate))\n",
    "    return model\n",
    "  def qlearn_to_play(self):\n",
    "    steps_list=[]\n",
    "    print('Total episodes:', self.num_episodes)\n",
    "    for i in range(self.num_episodes):\n",
    "      j=0   \n",
    "      s = self.env.reset()\n",
    "      done = False\n",
    "      self.eps *= self.decay_rate\n",
    "      while not done:\n",
    "          j+=1\n",
    "          Q = self.model.predict(s.reshape(-1, len(s)))\n",
    "          if np.random.random() < self.eps:\n",
    "              a = np.random.randint(0, 3)\n",
    "          else:\n",
    "              a = np.argmax(Q)\n",
    "          new_s, r, done, _ = self.env.step(a)\n",
    "          Q1 = self.model.predict(new_s.reshape(-1, len(s)))\n",
    "          target = abs(s[0]-new_s[0]) + self.gamma * np.max(Q1)\n",
    "          Q[0][a] = target\n",
    "          self.model.fit(s.reshape(-1, len(s)), Q.reshape(-1, 3), epochs=1, verbose=0)\n",
    "          s = new_s\n",
    "      print('Closing epsiode {} in {} steps'.format(i+1,j))\n",
    "      steps_list.append(j)\n",
    "    return steps_list\n",
    "  def eval_model(self,numevals=10):\n",
    "    c = 0\n",
    "    for _ in range(numevals):\n",
    "        s = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = np.argmax(self.model.predict(s.reshape(-1, len(s)))[0])\n",
    "            s,r,done,_ = self.env.step(a)\n",
    "            if s[0]>=0.5:            \n",
    "                c+=1                \n",
    "    print('Model is {}% accurate.'.format(c/numevals*100))\n",
    "      \n",
    "dql = dqnn(env)  \n",
    "steps_list = dql.qlearn_to_play()\n",
    "dql.eval_model()\n",
    "plt.plot(steps_list,'.')\n",
    "plt.xlabel('Number of training episodes')\n",
    "plt.ylabel('Number of steps to reach the flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of training episodes increases, the target is reached in more number of episodes. This can be considered as an evidence that our model is converging with the number of episodes.\n",
    "\n",
    "<img src=\"steps_episodes_v2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The following points may help us in designing a reinforcement problem:\n",
    "\n",
    "- In the problem at have do we have a suitable reward defined? For example, in this exercise the reward provided by OpenAI's gym is -1 till the car reaches the flag. This is not suitable to build a model. Hence, it was modified as the displacement from the current state.\n",
    "\n",
    "- Is it possible to generate a training set without having any constraints of memory and computational resources? Generating a training set and then building a model is fairly easier but in most of the situations this is going to be expensive.\n",
    "\n",
    "- Is it possible to discretize the states and build a table for choosing best action? With the right reward setup, building a table can be more efficient than using a regression model.\n",
    "\n",
    "- If we are unable to discretize the states then we have to build a regression model. Convergence of the model with suitable hyperparameters can sometimes be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
